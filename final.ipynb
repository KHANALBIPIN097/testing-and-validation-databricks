{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ac78229-c2df-4e81-97be-59f28f61ff54",
     "showTitle": true,
     "title": "# An Application for validation of tables in Databricks"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4208e12-bd3d-42df-bdd2-f4d673853dff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType, ArrayType\n",
    "from pyspark.sql.functions import col\n",
    "# creating the class(ReadingData) for reading the data from different source\n",
    "\n",
    "class ReadingData :\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.getOrCreate()\n",
    "        self.dataframe = None\n",
    "        self.exdf = None\n",
    "        # self.expected_schema = None\n",
    "    \n",
    "    def read_storage(self):\n",
    "        pass\n",
    "    def read_snowflake(self):\n",
    "        pass\n",
    "    def read_dbfs(self):\n",
    "        file_path = input(\"Enter the file path: \")\n",
    "        file_format = file_path.split('.')[-1].lower()\n",
    "\n",
    "        reader = self.spark.read.format(file_format)\n",
    "\n",
    "        if file_format == 'csv':\n",
    "            reader = reader.option('header', 'true').option('inferSchema', 'true')\n",
    "        elif file_format == 'parquet':\n",
    "            # No additional options required for Parquet format\n",
    "            pass\n",
    "        elif file_format == 'json':\n",
    "            \n",
    "            pass\n",
    "        elif file_format == 'text':\n",
    "            # Read text files as a whole file with one row per line\n",
    "            reader = reader.text()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "\n",
    "        self.dataframe = reader.load(file_path)\n",
    "        self.dataframe.show()\n",
    "        return self.dataframe\n",
    "    \n",
    "\n",
    "    \n",
    "    def get_meta_data(self):\n",
    "        print(\"======================================================================================\")\n",
    "        print(\" DataFrame MetaData \")\n",
    "        print(\"--------------------------------------------------------------------------------------\")\n",
    "        print(\"the schema of the DataFrame, including column names, data types, and nullable flags.\")\n",
    "        print(\"--------------------------------------------------------------------------------------\")\n",
    "        print(self.dataframe.printSchema())\n",
    "        print(\"--------------------------------------------------------------------------------------\")\n",
    "        print(\" the StructType object representing the schema of the DataFrame.\")\n",
    "        print(\"--------------------------------------------------------------------------------------\")\n",
    "        print(self.dataframe.schema)\n",
    "        print(\"--------------------------------------------------------------------------------------\")\n",
    "        print(\" a list of column names in the DataFrame.\")\n",
    "        print(\"--------------------------------------------------------------------------------------\")\n",
    "        print(self.dataframe.columns)\n",
    "        print(\"--------------------------------------------------------------------------------------\")\n",
    "        print(\" a list of (column_name, data_type) tuples for each column in the DataFrame\")\n",
    "        print(\"--------------------------------------------------------------------------------------\")\n",
    "        print(self.dataframe.dtypes)\n",
    "        print(\"======================================================================================\")\n",
    "\n",
    "class schema_validation(ReadingData):\n",
    "    \n",
    "\n",
    "\n",
    "    def take_expected_schema(self):\n",
    "        # taking expected schema from user as input \n",
    "        userinput = input('''how you wnat to input the expected schema for validation \n",
    "              enter 1.(D) for default 2. (F)for file 3.(T) TO enter through terminal''')\n",
    "\n",
    "        if userinput.upper() == \"D\":\n",
    "              self.expected_schema = StructType([\n",
    "                StructField(\"PassengerId\", IntegerType(), nullable=True),\n",
    "                StructField(\"Survived\", IntegerType(), nullable=True),\n",
    "                StructField(\"Pclass\", IntegerType(), nullable=True),\n",
    "                StructField(\"Name\", StringType(), nullable=True),\n",
    "                StructField(\"Sex\", StringType(), nullable=True),\n",
    "                StructField(\"Age\", DoubleType(), nullable=True),\n",
    "                StructField(\"SibSp\", IntegerType(), nullable=True),\n",
    "                StructField(\"Parch\", IntegerType(), nullable=True),\n",
    "                StructField(\"Ticket\", StringType(), nullable=True),\n",
    "                StructField(\"Fare\", DoubleType(), nullable=True),\n",
    "                StructField(\"Cabin\", StringType(), nullable=True),\n",
    "                StructField(\"Embarked\", StringType(), nullable=True)\n",
    "            ])\n",
    "        \n",
    "        elif userinput.upper() == \"F\":\n",
    "            file_path = input(\"Enter the file path: \")\n",
    "            file_format = file_path.split('.')[-1].lower()\n",
    "\n",
    "            reader = self.spark.read.format(file_format)\n",
    "\n",
    "            if file_format == 'csv':\n",
    "                reader = reader.option('header', 'true').option('inferSchema', 'true')\n",
    "            elif file_format == 'parquet':\n",
    "                # No additional options required for Parquet format\n",
    "                pass\n",
    "            elif file_format == 'json':\n",
    "                \n",
    "                pass\n",
    "            elif file_format == 'text':\n",
    "                # Read text files as a whole file with one row per line\n",
    "                reader = reader.text()\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "            self.exdf = reader.load(file_path)\n",
    "            self.expected_schema = self.exdf.schema\n",
    "        elif userinput.upper() == \"T\":\n",
    "            self.expected_schema = StructType()\n",
    "            num_fields = int(input(\"Enter the number of fields in the expected schema: \"))\n",
    "\n",
    "            for i in range(num_fields):\n",
    "                field_name = input(f\"Enter the name for field {i + 1}: \")\n",
    "                field_type = input(f\"Enter the data type for field {i + 1}: \")\n",
    "                nullable = input(f\"Is field {i + 1} nullable? (y/n): \").lower()\n",
    "\n",
    "                if field_type.lower() == \"string\":\n",
    "                    data_type = StringType()\n",
    "                elif field_type.lower() == \"integer\":\n",
    "                    data_type = IntegerType()\n",
    "                elif field_type.lower() == \"double\":\n",
    "                    data_type = DoubleType()\n",
    "                elif field_type.lower() == \"array\":\n",
    "                    element_type = input(f\"Enter the element data type for the array field {i + 1}: \")\n",
    "                    if element_type.lower() == \"string\":\n",
    "                        data_type = ArrayType(StringType())\n",
    "                    elif element_type.lower() == \"integer\":\n",
    "                        data_type = ArrayType(IntegerType())\n",
    "                    elif element_type.lower() == \"double\":\n",
    "                        data_type = ArrayType(DoubleType())\n",
    "                    else:\n",
    "                        print(f\"Invalid element data type entered for the array field {i + 1}. Using ArrayType(StringType) as default.\")\n",
    "                        data_type = ArrayType(StringType())\n",
    "                else:\n",
    "                    print(f\"Invalid data type entered for field {i + 1}. Using StringType as default.\")\n",
    "                    data_type = StringType()\n",
    "\n",
    "                if nullable == \"y\":\n",
    "                    is_nullable = True\n",
    "                else:\n",
    "                    is_nullable = False\n",
    "                \n",
    "                field = StructField(field_name, data_type, \n",
    "                nullable=is_nullable)\n",
    "                self.expected_schema.add(field)\n",
    "        else: \n",
    "            print(\"enter among given option\")\n",
    "\n",
    "            # Compare actual DataFrame schema with expected schema\n",
    "    def validat_schema (self):\n",
    "            actual_schema = self.dataframe.schema\n",
    "\n",
    "            if actual_schema == self.expected_schema:\n",
    "                print(\"Schema validation passed.\")\n",
    "            else:\n",
    "                print(\"Schema validation failed.\")\n",
    "                print(\"Expected Schema:\")\n",
    "                print (self.expected_schema)\n",
    "                print(\"Actual Schema:\")\n",
    "                self.dataframe.printSchema()\n",
    "\n",
    "class checking_tables():\n",
    "    def __init__(self,dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "\n",
    "    def check_table(self):\n",
    "            #  converting dataframe table view name as \"my_table\"\n",
    "            self.dataframe.createOrReplaceTempView(\"my_table\")\n",
    "            \n",
    "            # Define tables to validate\n",
    "            tables = [\"my_table\"]\n",
    "\n",
    "            # Perform validation for each table\n",
    "            for table in tables:\n",
    "                \n",
    "                    # Read table data\n",
    "                    df = spark.sql(f\"SELECT * FROM {table}\")\n",
    "                    \n",
    "                    # Perform validation checks\n",
    "                    row_count = df.count()\n",
    "                    column_count = len(df.columns)\n",
    "                    has_null_values = df.rdd.flatMap(lambda row: row).map(lambda x: x is None).sum()\n",
    "                    \n",
    "\n",
    "                # Iterate over each column and calculate the percentage of null values\n",
    "                    for column in df.columns:\n",
    "                        null_count = df.filter(col(column).isNull()).count()\n",
    "                        null_percentage = (null_count / row_count) * 100\n",
    "                        print(f\"Column '{column}': {null_percentage:.2f}% null\")\n",
    "                        print(\"-----------------------------------------------------\")\n",
    "                        \n",
    "                    \n",
    "                    # Print validation results\n",
    "                    print(f\"Validation Results for Table: {table}\")\n",
    "                    print(f\"Total Rows: {row_count}\")\n",
    "                    print(f\"Total Columns: {column_count}\")\n",
    "                    print(f\"Null Values: {has_null_values}\")\n",
    "                    \n",
    "                    print(\"---\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87e42000-8ccd-49d4-a005-9d8f4c1392ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter the file path:  /FileStore/tables/titanic.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\nonly showing top 20 rows\n\n======================================================================================\n DataFrame MetaData \n--------------------------------------------------------------------------------------\nthe schema of the DataFrame, including column names, data types, and nullable flags.\n--------------------------------------------------------------------------------------\nroot\n |-- PassengerId: integer (nullable = true)\n |-- Survived: integer (nullable = true)\n |-- Pclass: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: integer (nullable = true)\n |-- Parch: integer (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Cabin: string (nullable = true)\n |-- Embarked: string (nullable = true)\n\nNone\n--------------------------------------------------------------------------------------\n the StructType object representing the schema of the DataFrame.\n--------------------------------------------------------------------------------------\nStructType([StructField('PassengerId', IntegerType(), True), StructField('Survived', IntegerType(), True), StructField('Pclass', IntegerType(), True), StructField('Name', StringType(), True), StructField('Sex', StringType(), True), StructField('Age', DoubleType(), True), StructField('SibSp', IntegerType(), True), StructField('Parch', IntegerType(), True), StructField('Ticket', StringType(), True), StructField('Fare', DoubleType(), True), StructField('Cabin', StringType(), True), StructField('Embarked', StringType(), True)])\n--------------------------------------------------------------------------------------\n a list of column names in the DataFrame.\n--------------------------------------------------------------------------------------\n['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n--------------------------------------------------------------------------------------\n a list of (column_name, data_type) tuples for each column in the DataFrame\n--------------------------------------------------------------------------------------\n[('PassengerId', 'int'), ('Survived', 'int'), ('Pclass', 'int'), ('Name', 'string'), ('Sex', 'string'), ('Age', 'double'), ('SibSp', 'int'), ('Parch', 'int'), ('Ticket', 'string'), ('Fare', 'double'), ('Cabin', 'string'), ('Embarked', 'string')]\n======================================================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "how you wnat to input the expected schema for validation \n",
       "              enter 1.(D) for default 2. (F)for file 3.(T) TO enter through terminal d"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema validation passed.\nColumn 'PassengerId': 0.00% null\n-----------------------------------------------------\nColumn 'Survived': 0.00% null\n-----------------------------------------------------\nColumn 'Pclass': 0.00% null\n-----------------------------------------------------\nColumn 'Name': 0.00% null\n-----------------------------------------------------\nColumn 'Sex': 0.00% null\n-----------------------------------------------------\nColumn 'Age': 19.87% null\n-----------------------------------------------------\nColumn 'SibSp': 0.00% null\n-----------------------------------------------------\nColumn 'Parch': 0.00% null\n-----------------------------------------------------\nColumn 'Ticket': 0.00% null\n-----------------------------------------------------\nColumn 'Fare': 0.00% null\n-----------------------------------------------------\nColumn 'Cabin': 77.10% null\n-----------------------------------------------------\nColumn 'Embarked': 0.22% null\n-----------------------------------------------------\nValidation Results for Table: my_table\nTotal Rows: 891\nTotal Columns: 12\nNull Values: 866\n---\n\n"
     ]
    }
   ],
   "source": [
    "getData = schema_validation()\n",
    "\n",
    "# print(\"define the source you want to get datafrom\")\n",
    "# filedes = input(\"enter the 1. (F) for snowflake 2. (D) for dbfs 3. (S) from storage\")\n",
    "# if filedes.upper == 'F':\n",
    "    # getData.read_snowflake()\n",
    "# elif filedes.upper == 'D':\n",
    "dataframe  = getData.read_dbfs()\n",
    "\n",
    "# elif filedes.upper == 'S':\n",
    "#     getData.read_storage()\n",
    "# else:\n",
    "#     print(\"please enter through the given source\")\n",
    "\n",
    "getData.get_meta_data()\n",
    "    \n",
    "getData.take_expected_schema()\n",
    "getData.validat_schema()\n",
    "\n",
    "\n",
    "table_info = checking_tables(dataframe)\n",
    "table_info.check_table()\n",
    "\n",
    "\n",
    "# file_path = \"/FileStore/tables/titanic.csv\"Read any type of file format and convert it into pyspark dataframe \n",
    "# file_path =\"/FileStore/tables/sample1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e92d83a-df63-4ec2-8a1f-37b10722aa19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "final",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
